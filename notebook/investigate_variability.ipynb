{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Iteration variance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_table(experiment, col='overall_accuracy'):\n",
    "    file_name = f'../data/evaluation/{experiment}_eval_table.csv'\n",
    "    evaluation_df = pd.read_csv(file_name)\n",
    "    evaluation_df['overall_accuracy'] = evaluation_df['accuracy'].fillna(0)\n",
    "    evaluation_df['experiment'] = experiment\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  completeness  count\n",
      "0            3   7837\n",
      "1            0   1668\n",
      "2            2    712\n",
      "3            1    454\n",
      "  overall_accuracy  count\n",
      "0              0.0   9433\n",
      "1              3.0    897\n",
      "2              1.0    183\n",
      "3              2.0    158\n",
      "  structural_compliance  count\n",
      "0                     0   4898\n",
      "1                     3   4347\n",
      "2                     1    714\n",
      "3                     2    712\n"
     ]
    }
   ],
   "source": [
    "for col in ['completeness','overall_accuracy', 'structural_compliance']:\n",
    "    file_names = ['gpt', 'fewshot', 'llama2', 'newgpt','rag']\n",
    "    eval_dfs = [get_eval_table(file_name, col) for file_name in file_names]\n",
    "    eval_table_df = pd.concat(eval_dfs)\n",
    "    evaluation_var_df = eval_table_df.groupby(['sample_id','top_n', 'prompt','gpt_version','input_type','experiment'])[col].sum().reset_index()\n",
    "    print(evaluation_var_df[col].astype(str).value_counts().reset_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakdown by methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'diff' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'diff' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
      "/var/folders/jt/xqkngjbx2vj5dhrgmccfc01m0000gp/T/ipykernel_73521/772243224.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'diff' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n"
     ]
    }
   ],
   "source": [
    "for col in ['completeness','overall_accuracy', 'structural_compliance']:\n",
    "    file_names = ['gpt', 'fewshot', 'llama2', 'newgpt','rag']\n",
    "    eval_dfs = [get_eval_table(file_name, col) for file_name in file_names]\n",
    "    eval_table_df = pd.concat(eval_dfs)\n",
    "    evaluation_var_df = eval_table_df.groupby(['sample_id','top_n', 'prompt','gpt_version','input_type','experiment'])[col].sum().reset_index()\n",
    "    # for col = 1 or 2, we want to change the value to 'diff'\n",
    "    evaluation_var_df[col].iloc[evaluation_var_df[col].isin([1,2])] = 'diff'\n",
    "    \n",
    "    variants_count_df = evaluation_var_df.groupby(['gpt_version','experiment','top_n','prompt','input_type',col]).size().reset_index()\n",
    "    variants_count_df.rename(columns={0:'col_count'}, inplace=True)\n",
    "    sum_count_df = evaluation_var_df.groupby(['gpt_version','experiment','top_n','prompt','input_type']).size().reset_index()\n",
    "    sum_count_df.rename(columns={0:'sum_count'},inplace=True)\n",
    "    nonzero_count_df = evaluation_var_df[evaluation_var_df[col]!=0].groupby(['gpt_version','experiment','top_n','prompt','input_type']).size().reset_index()\n",
    "    nonzero_count_df.rename(columns={0:'nonzero_count'},inplace=True)\n",
    "    merged_df = variants_count_df.merge(sum_count_df).merge(nonzero_count_df)\n",
    "    merged_df['difference_ratio'] = (merged_df['col_count'])/merged_df['sum_count']\n",
    "    merged_df['nonzero_difference_ratio'] = (merged_df['col_count'])/merged_df['nonzero_count']\n",
    "    # for col_count = 2 or 3, we want to add the ratio together\n",
    "    merged_df = merged_df[merged_df[col]=='diff']\n",
    "    # drop col\n",
    "    merged_df.drop(col, axis=1, inplace=True)\n",
    "    merged_df.sort_values('nonzero_difference_ratio', ascending=False, inplace=True)\n",
    "    merged_df.to_csv(f'../data/evaluation/{col}_difference_ratio.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate Calendar Month Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   completeness_abs_diff    0\n",
      "0                    0.0  211\n",
      "1                    1.0   11\n",
      "2                    2.0    7\n",
      "3                    3.0   21\n",
      "   overall_accuracy_abs_diff    0\n",
      "0                        0.0  240\n",
      "1                        1.0    2\n",
      "2                        2.0    3\n",
      "3                        3.0    5\n",
      "   structural_compliance_abs_diff    0\n",
      "0                             0.0  125\n",
      "1                             1.0    5\n",
      "2                             2.0    2\n",
      "3                             3.0  118\n"
     ]
    }
   ],
   "source": [
    "for col in ['completeness','overall_accuracy', 'structural_compliance']:\n",
    "    file_names = ['gpt', 'newgpt']\n",
    "    eval_dfs = [get_eval_table(file_name, col) for file_name in file_names]\n",
    "    eval_table_df = pd.concat(eval_dfs)\n",
    "    # pivot table from long to wide by experiment\n",
    "    pivot_df = eval_table_df.pivot_table(index=['sample_id','top_n','prompt','gpt_version','input_type','iteration'], columns='experiment', values=col).reset_index()\n",
    "    pivot_df = pivot_df[~pivot_df['newgpt'].isna()]\n",
    "    pivot_df = pivot_df[~pivot_df['gpt'].isna()]\n",
    "    gpt_evaluation_var_df = pivot_df.groupby(['sample_id','top_n', 'prompt','gpt_version','input_type'])['gpt'].sum().reset_index()\n",
    "    newgpt_evaluation_var_df = pivot_df.groupby(['sample_id','top_n', 'prompt','gpt_version','input_type'])['newgpt'].sum().reset_index()\n",
    "    merged_df = gpt_evaluation_var_df.merge(newgpt_evaluation_var_df)\n",
    "    merged_df[f'{col}_abs_diff'] = abs(merged_df['gpt'] - merged_df['newgpt'])\n",
    "    print(merged_df.groupby([f'{col}_abs_diff']).size().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(completeness_variability_df[0] == 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_diff</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   abs_diff    0\n",
       "0       0.0  125\n",
       "1       1.0    5\n",
       "2       2.0    2\n",
       "3       3.0  118"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>experiment</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>top_n</th>\n",
       "      <th>prompt</th>\n",
       "      <th>gpt_version</th>\n",
       "      <th>input_type</th>\n",
       "      <th>iteration</th>\n",
       "      <th>gpt</th>\n",
       "      <th>newgpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12594</th>\n",
       "      <td>Init_example_1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>free_text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>Init_example_1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>free_text</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>Init_example_1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>free_text</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>Init_example_1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>free_text</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>Init_example_1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>free_text</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18547</th>\n",
       "      <td>PMID30709875.4</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>free_text</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18548</th>\n",
       "      <td>PMID30709875.4</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>free_text</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18549</th>\n",
       "      <td>PMID30709875.4</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>free_text</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18550</th>\n",
       "      <td>PMID30709875.4</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>free_text</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18551</th>\n",
       "      <td>PMID30709875.4</td>\n",
       "      <td>10</td>\n",
       "      <td>d</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>free_text</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "experiment         sample_id  top_n prompt    gpt_version input_type  \\\n",
       "12594       Init_example_1.0     10      d  gpt-3.5-turbo  free_text   \n",
       "12595       Init_example_1.0     10      d  gpt-3.5-turbo  free_text   \n",
       "12596       Init_example_1.0     10      d  gpt-3.5-turbo  free_text   \n",
       "12597       Init_example_1.0     10      d          gpt-4  free_text   \n",
       "12598       Init_example_1.0     10      d          gpt-4  free_text   \n",
       "...                      ...    ...    ...            ...        ...   \n",
       "18547         PMID30709875.4     10      d  gpt-3.5-turbo  free_text   \n",
       "18548         PMID30709875.4     10      d  gpt-3.5-turbo  free_text   \n",
       "18549         PMID30709875.4     10      d          gpt-4  free_text   \n",
       "18550         PMID30709875.4     10      d          gpt-4  free_text   \n",
       "18551         PMID30709875.4     10      d          gpt-4  free_text   \n",
       "\n",
       "experiment  iteration  gpt  newgpt  \n",
       "12594               1  0.0     1.0  \n",
       "12595               2  0.0     1.0  \n",
       "12596               3  0.0     1.0  \n",
       "12597               1  1.0     1.0  \n",
       "12598               2  1.0     1.0  \n",
       "...               ...  ...     ...  \n",
       "18547               2  0.0     1.0  \n",
       "18548               3  0.0     1.0  \n",
       "18549               1  1.0     1.0  \n",
       "18550               2  1.0     1.0  \n",
       "18551               3  1.0     1.0  \n",
       "\n",
       "[750 rows x 8 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
